{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pprint\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: stale element not found in the current frame\n  (Session info: chrome=124.0.6367.210); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n\tGetHandleVerifier [0x00FBC113+48259]\n\t(No symbol) [0x00F4CA41]\n\t(No symbol) [0x00E40A17]\n\t(No symbol) [0x00E45B58]\n\t(No symbol) [0x00E47652]\n\t(No symbol) [0x00E476D0]\n\t(No symbol) [0x00E80A0B]\n\t(No symbol) [0x00E80C9B]\n\t(No symbol) [0x00E77211]\n\t(No symbol) [0x00EA0DE4]\n\t(No symbol) [0x00E76E25]\n\t(No symbol) [0x00EA1034]\n\t(No symbol) [0x00EB9B9C]\n\t(No symbol) [0x00EA0B36]\n\t(No symbol) [0x00E7570D]\n\t(No symbol) [0x00E762CD]\n\tGetHandleVerifier [0x012765A3+2908435]\n\tGetHandleVerifier [0x012B3BBB+3159851]\n\tGetHandleVerifier [0x010550CB+674875]\n\tGetHandleVerifier [0x0105B28C+699900]\n\t(No symbol) [0x00F56244]\n\t(No symbol) [0x00F52298]\n\t(No symbol) [0x00F5242C]\n\t(No symbol) [0x00F44BB0]\n\tBaseThreadInitThunk [0x7720FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77427CBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77427C8E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Looping through every row of every column and creating the list of values associated with each feature\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tables\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mTAG_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     cells \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cells):\n\u001b[0;32m     94\u001b[0m         retry_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initialize retry count for this cell\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\misol\\anaconda3\\envs\\grainger_scrapping_env\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:439\u001b[0m, in \u001b[0;36mWebElement.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    436\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    437\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_CHILD_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\misol\\anaconda3\\envs\\grainger_scrapping_env\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\misol\\anaconda3\\envs\\grainger_scrapping_env\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\misol\\anaconda3\\envs\\grainger_scrapping_env\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: stale element not found in the current frame\n  (Session info: chrome=124.0.6367.210); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n\tGetHandleVerifier [0x00FBC113+48259]\n\t(No symbol) [0x00F4CA41]\n\t(No symbol) [0x00E40A17]\n\t(No symbol) [0x00E45B58]\n\t(No symbol) [0x00E47652]\n\t(No symbol) [0x00E476D0]\n\t(No symbol) [0x00E80A0B]\n\t(No symbol) [0x00E80C9B]\n\t(No symbol) [0x00E77211]\n\t(No symbol) [0x00EA0DE4]\n\t(No symbol) [0x00E76E25]\n\t(No symbol) [0x00EA1034]\n\t(No symbol) [0x00EB9B9C]\n\t(No symbol) [0x00EA0B36]\n\t(No symbol) [0x00E7570D]\n\t(No symbol) [0x00E762CD]\n\tGetHandleVerifier [0x012765A3+2908435]\n\tGetHandleVerifier [0x012B3BBB+3159851]\n\tGetHandleVerifier [0x010550CB+674875]\n\tGetHandleVerifier [0x0105B28C+699900]\n\t(No symbol) [0x00F56244]\n\t(No symbol) [0x00F52298]\n\t(No symbol) [0x00F5242C]\n\t(No symbol) [0x00F44BB0]\n\tBaseThreadInitThunk [0x7720FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77427CBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77427C8E+238]\n"
     ]
    }
   ],
   "source": [
    "# Opening web browser\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Getting the the page we want to scrap\n",
    "driver.get('https://www.grainger.com/category/lighting/light-bulbs-lamps/light-bulb-lamp-accessories-changers?categoryIndex=9')\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "# Extracting page main title\n",
    "title_page = driver.find_element(By.CSS_SELECTOR, '.Gb4id').text\n",
    "# Identification of each sections of the page. Creating an empty dictionary to save the data.\n",
    "sections = driver.find_elements(By.CLASS_NAME, 'MAcbb-')\n",
    "section_dict = {}\n",
    "\n",
    "# Looping throught each section to get the information we want which are the tables\n",
    "for section in sections:\n",
    "    # Extracting section title\n",
    "    title_section = section.find_element(By.CLASS_NAME, \"sC0Aof\").text\n",
    "    sub_section_dict = {}\n",
    "    section_dict[title_section] = sub_section_dict\n",
    "\n",
    "    # if there is sub section, we looping throught ever sub-section to extract every table\n",
    "    if section.find_elements(By.CLASS_NAME, \"T8G8vu\"):\n",
    "        sub_sections = section.find_elements(By.CLASS_NAME, \"T8G8vu\")\n",
    "        extra_title = ''\n",
    "        for sub_section in sub_sections:\n",
    "\n",
    "            try:\n",
    "                title_sub_section = extra_title + \\\n",
    "                    sub_section.find_element(By.CLASS_NAME, 'SQoGqa').text\n",
    "                sub_section_dict[title_sub_section] = {}\n",
    "\n",
    "                # To load the HTML, we need to 'activate' the JS by clicking on a button of the table\n",
    "                sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "                # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "                features = sub_section.find_element(By.CLASS_NAME, 'JxT10f')\n",
    "                feature_list = (features.text).split('\\n')\n",
    "\n",
    "                # Getting the table associated with the sub section\n",
    "                tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "                feature_dict = {}\n",
    "\n",
    "                # Adding the date of scrapping\n",
    "                current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # Going through every row of every column and creating the list of values associated with each feature\n",
    "                for row in tables.find_elements(By.TAG_NAME, 'tr'):\n",
    "                    cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                    for i, cell in enumerate(cells):\n",
    "                        # Extract text from each cell and add to feature_dict\n",
    "                        value = cell.text\n",
    "                        if feature_list[i] not in feature_dict:\n",
    "                            feature_dict[feature_list[i]] = []\n",
    "                        feature_dict[feature_list[i]].append(value)\n",
    "                        # Add date\n",
    "                        if 'Date' not in feature_dict:\n",
    "                            feature_dict['Date'] = []\n",
    "                        feature_dict['Date'].append(current_date)\n",
    "\n",
    "                sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "            except Exception as e:\n",
    "                extra_title = sub_section.find_element(\n",
    "                    By.CLASS_NAME, 'SQoGqa').text + ' '\n",
    "                del sub_section_dict[title_sub_section]\n",
    "\n",
    "    else:\n",
    "\n",
    "        section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "        # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "        features = section.find_element(By.CLASS_NAME, 'JxT10f')\n",
    "        feature_list = (features.text).split('\\n')\n",
    "\n",
    "        # Getting the table associated with the sub section\n",
    "        tables = section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "        feature_dict = {}\n",
    "\n",
    "        # Adding the date of scrapping\n",
    "        current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        import time  # Import the time module for adding delays\n",
    "\n",
    "        # Looping through every row of every column and creating the list of values associated with each feature\n",
    "        for row in tables.find_elements(By.TAG_NAME, 'tr'):\n",
    "            cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "            for i, cell in enumerate(cells):\n",
    "                retry_count = 0  # Initialize retry count for this cell\n",
    "                MAX_RETRIES = 3  # Maximum number of retries\n",
    "                while retry_count < MAX_RETRIES:\n",
    "                    try:\n",
    "                        value = cell.text  # Attempt to retrieve the text of the cell\n",
    "                        if feature_list[i] not in feature_dict:\n",
    "                            feature_dict[feature_list[i]] = []\n",
    "                        feature_dict[feature_list[i]].append(value)\n",
    "                        # Add date\n",
    "                        if 'Date' not in feature_dict:\n",
    "                            feature_dict['Date'] = []\n",
    "                        feature_dict['Date'].append(current_date)\n",
    "                        break  # Break out of the retry loop if successful\n",
    "                    except StaleElementReferenceException:\n",
    "                        # Increment retry count and add a small delay before retrying\n",
    "                        retry_count += 1\n",
    "                        time.sleep(1)\n",
    "                else:\n",
    "                    # Handle the case when max retries are reached\n",
    "                    print(\"Max retries reached. Unable to retrieve cell text.\")\n",
    "        \n",
    "        sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "\n",
    "page_dict = {}\n",
    "page_dict[title_page] = section_dict\n",
    "\n",
    "pprint.pprint(page_dict)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_scraper(url):\n",
    "\n",
    "    # Opening web browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(\n",
    "        ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Getting the the page we want to scrap\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    # Extracting page main title\n",
    "    title_page = driver.find_element(By.CSS_SELECTOR, '.Gb4id').text\n",
    "    # Identification of each sections of the page. Creating an empty dictionary to save the data.\n",
    "    sections = driver.find_elements(By.CLASS_NAME, 'MAcbb-')\n",
    "    section_dict = {}\n",
    "\n",
    "    # Looping throught each section to get the information we want which are the tables\n",
    "    for section in sections:\n",
    "        # Extracting section title\n",
    "        title_section = section.find_element(By.CLASS_NAME, \"sC0Aof\").text\n",
    "        sub_section_dict = {}\n",
    "        section_dict[title_section] = sub_section_dict\n",
    "\n",
    "        # Looping throught ever sub-section to extract every table\n",
    "        if section.find_elements(By.CLASS_NAME, \"T8G8vu\"):\n",
    "            sub_sections = section.find_elements(By.CLASS_NAME, \"T8G8vu\")\n",
    "            extra_title = ''\n",
    "            for sub_section in sub_sections:\n",
    "\n",
    "                try:\n",
    "                    title_sub_section = extra_title + \\\n",
    "                        sub_section.find_element(By.CLASS_NAME, 'SQoGqa').text\n",
    "                    sub_section_dict[title_sub_section] = {}\n",
    "\n",
    "                    # To load the HTML, we need to 'activate' the JS by clicking on a button of the table\n",
    "                    sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "                    # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "                    features = sub_section.find_element(\n",
    "                        By.CLASS_NAME, 'JxT10f')\n",
    "                    feature_list = (features.text).split('\\n')\n",
    "\n",
    "                    # Getting the table associated with the sub section\n",
    "                    tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "                    feature_dict = {}\n",
    "\n",
    "                    # Adding the date of scrapping\n",
    "                    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                    # Going through every row of every column and creating the list of values associated with each feature\n",
    "                    rows = tables.find_elements(By.TAG_NAME, 'tr')\n",
    "                    for i in range(len(feature_list)):\n",
    "                        feature_dict[feature_list[i]] = []\n",
    "                        feature_dict['Date'] = []\n",
    "\n",
    "                        for row in rows:\n",
    "                            value = row.find_element(\n",
    "                                By.XPATH, f'./td[{i+1}]').text\n",
    "                            feature_dict[feature_list[i]].append(value)\n",
    "                            feature_dict['Date'].append(current_date)\n",
    "\n",
    "                    sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "                except Exception as e:\n",
    "                    extra_title = sub_section.find_element(\n",
    "                        By.CLASS_NAME, 'SQoGqa').text + ' '\n",
    "                    del sub_section_dict[title_sub_section]\n",
    "\n",
    "        else:\n",
    "\n",
    "            sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "            # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "            features = sub_section.find_element(By.CLASS_NAME, 'JxT10f')\n",
    "            feature_list = (features.text).split('\\n')\n",
    "\n",
    "            # Getting the table associated with the sub section\n",
    "            tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "            feature_dict = {}\n",
    "\n",
    "            # Adding the date of scrapping\n",
    "            current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Going through every row of every column and creating the list of values associated with each feature\n",
    "            rows = tables.find_elements(By.TAG_NAME, 'tr')\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dict[feature_list[i]] = []\n",
    "                feature_dict['Date'] = []\n",
    "\n",
    "                for row in rows:\n",
    "                    value = row.find_element(By.XPATH, f'./td[{i+1}]').text\n",
    "                    feature_dict[feature_list[i]].append(value)\n",
    "                    feature_dict['Date'].append(current_date)\n",
    "\n",
    "            sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "    page_dict = {}\n",
    "    page_dict[title_page] = section_dict\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return page_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_scrapped(urls):\n",
    "    data = []\n",
    "    # urls_done has been created if the script come to a stop for whatever reason. We can look at the last link added here and restart from here.\n",
    "    urls_done = []\n",
    "    for url in urls:\n",
    "        print(url)  # Help to follow the script live\n",
    "        try:\n",
    "            scraping = url_scraper(url)\n",
    "            data.append(scraping)\n",
    "            urls_done.append(url)\n",
    "        except:  # If there is an issue on a url we can just skip it and move on\n",
    "            pass\n",
    "\n",
    "    return data, urls_done[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining urls we want to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls_extractor/urls_extractor/spiders/urls.json', 'r') as file:\n",
    "    datas = json.load(file)\n",
    "\n",
    "urls_to_scrap = []\n",
    "\n",
    "for data in datas:\n",
    "    for key, value in data.items():\n",
    "        urls_to_scrap.append(value)\n",
    "\n",
    "u = urls_to_scrap[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/circular-light-bulbs-lamps?categoryIndex=8\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/light-bulb-lamp-accessories-changers?categoryIndex=9\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/u-bend-light-bulbs-lamps?categoryIndex=7\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/linear-light-bulbs-lamps?categoryIndex=1\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/spot-reflector-flood-light-bulbs-lamps?categoryIndex=3\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/plug-in-cfl-led-light-bulbs-lamps?categoryIndex=6\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/high-output-hid-led-light-bulbs-lamps?categoryIndex=4\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/standard-decorative-light-bulbs-lamps?categoryIndex=2\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/spade-wire-terminal-base-miniature-light-bulbs-lamps?categoryIndex=10\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/festoon-cap-loop-base-miniature-light-bulbs-lamps?categoryIndex=11\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/telephone-slide-base-miniature-light-bulbs-lamps?categoryIndex=6\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/r7s-base-miniature-light-bulbs-lamps?categoryIndex=7\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/2-pin-base-miniature-light-bulbs-lamps?categoryIndex=5\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/printed-circuit-socket-miniature-light-bulbs-lamps?categoryIndex=9\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/screw-base-miniature-light-bulbs-lamps?categoryIndex=1\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/prefocus-flanged-base-miniature-light-bulbs-lamps?categoryIndex=8\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/bayonet-base-miniature-light-bulbs-lamps?categoryIndex=4\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/wedge-base-miniature-light-bulbs-lamps?categoryIndex=3\n",
      "https://www.grainger.com/category/lighting/light-bulbs-lamps/miniature-light-bulbs-lamps/flanged-grooved-base-miniature-light-bulbs-lamps?categoryIndex=2\n"
     ]
    }
   ],
   "source": [
    "results = urls_scrapped(urls_to_scrap)\n",
    "scraping = results[0]\n",
    "urls_processed = results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json__saving(page_dict):\n",
    "    json_string = json.dumps(page_dict)\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    with open(f'scrapings/{current_date}_mon_fichier.json', 'w') as datas:\n",
    "        datas.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "json__saving(scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we should save scraping to json and save this json on the data base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
