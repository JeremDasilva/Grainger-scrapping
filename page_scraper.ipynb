{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening web browser\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(\n",
    "    ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Getting the the page we want to scrap\n",
    "driver.get('https://www.grainger.com/category/lighting/light-bulbs-lamps/circular-light-bulbs-lamps?categoryIndex=8')\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "# Extracting page main title\n",
    "title_page = driver.find_element(By.CLASS_NAME, 'KkejIK').text\n",
    "# Identification of each sections of the page. Creating an empty dictionary to save the data.\n",
    "sections = driver.find_elements(By.CLASS_NAME, 'MAcbb-')\n",
    "section_dict = {}\n",
    "\n",
    "# Looping throught each section to get the information we want which are the tables\n",
    "for section in sections:\n",
    "    # Extracting section title\n",
    "    title_section = section.find_element(By.CLASS_NAME, \"sC0Aof\").text\n",
    "    sub_section_dict = {}\n",
    "    section_dict[title_section] = sub_section_dict\n",
    "\n",
    "    # if there is sub section, we looping throught ever sub-section to extract every table\n",
    "    if section.find_elements(By.CLASS_NAME, \"T8G8vu\"):\n",
    "        sub_sections = section.find_elements(By.CLASS_NAME, \"T8G8vu\")\n",
    "        extra_title = ''\n",
    "        for sub_section in sub_sections:\n",
    "\n",
    "            try:\n",
    "                title_sub_section = extra_title + \\\n",
    "                    sub_section.find_element(By.CLASS_NAME, 'SQoGqa').text\n",
    "                sub_section_dict[title_sub_section] = {}\n",
    "\n",
    "                # To load the HTML, we need to 'activate' the JS by clicking on a button of the table\n",
    "                sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "                # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "                features = sub_section.find_element(By.CLASS_NAME, 'JxT10f')\n",
    "                feature_list = (features.text).split('\\n')\n",
    "\n",
    "                # Getting the table associated with the sub section\n",
    "                tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "                feature_dict = {}\n",
    "\n",
    "                # Adding the date of scrapping\n",
    "                current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # Going through every row of every column and creating the list of values associated with each feature\n",
    "                rows = tables.find_elements(By.TAG_NAME, 'tr')\n",
    "                for i in range(len(feature_list)):\n",
    "                    feature_dict[feature_list[i]] = []\n",
    "                    feature_dict['Date'] = []\n",
    "\n",
    "                    for row in rows:\n",
    "                        value = row.find_element(By.XPATH, f'./td[{i+1}]').text\n",
    "                        feature_dict[feature_list[i]].append(value)\n",
    "                        feature_dict['Date'].append(current_date)\n",
    "\n",
    "                sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "            except Exception as e:\n",
    "                extra_title = sub_section.find_element(\n",
    "                    By.CLASS_NAME, 'SQoGqa').text + ' '\n",
    "                del sub_section_dict[title_sub_section]\n",
    "\n",
    "    else:\n",
    "\n",
    "        sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "        # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "        features = sub_section.find_element(By.CLASS_NAME, 'JxT10f')\n",
    "        feature_list = (features.text).split('\\n')\n",
    "\n",
    "        # Getting the table associated with the sub section\n",
    "        tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "        feature_dict = {}\n",
    "\n",
    "        # Adding the date of scrapping\n",
    "        current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Going through every row of every column and creating the list of values associated with each feature\n",
    "        rows = tables.find_elements(By.TAG_NAME, 'tr')\n",
    "        for i in range(len(feature_list)):\n",
    "            feature_dict[feature_list[i]] = []\n",
    "            feature_dict['Date'] = []\n",
    "\n",
    "            for row in rows:\n",
    "                value = row.find_element(By.XPATH, f'./td[{i+1}]').text\n",
    "                feature_dict[feature_list[i]].append(value)\n",
    "                feature_dict['Date'].append(current_date)\n",
    "\n",
    "        sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "\n",
    "page_dict = {}\n",
    "page_dict[title_page] = section_dict\n",
    "\n",
    "pprint.pprint(page_dict)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_scraper(url):\n",
    "\n",
    "    # Opening web browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(\n",
    "        ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Getting the the page we want to scrap\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    # Extracting page main title\n",
    "    title_page = driver.find_element(By.CLASS_NAME, 'KkejIK').text\n",
    "    # Identification of each sections of the page. Creating an empty dictionary to save the data.\n",
    "    sections = driver.find_elements(By.CLASS_NAME, 'MAcbb-')\n",
    "    section_dict = {}\n",
    "\n",
    "    # Looping throught each section to get the information we want which are the tables\n",
    "    for section in sections:\n",
    "        # Extracting section title\n",
    "        title_section = section.find_element(By.CLASS_NAME, \"sC0Aof\").text\n",
    "        sub_section_dict = {}\n",
    "        section_dict[title_section] = sub_section_dict\n",
    "\n",
    "        # Looping throught ever sub-section to extract every table\n",
    "        if section.find_elements(By.CLASS_NAME, \"T8G8vu\"):\n",
    "            sub_sections = section.find_elements(By.CLASS_NAME, \"T8G8vu\")\n",
    "            extra_title = ''\n",
    "            for sub_section in sub_sections:\n",
    "\n",
    "                try:\n",
    "                    title_sub_section = extra_title + \\\n",
    "                        sub_section.find_element(By.CLASS_NAME, 'SQoGqa').text\n",
    "                    sub_section_dict[title_sub_section] = {}\n",
    "\n",
    "                    # To load the HTML, we need to 'activate' the JS by clicking on a button of the table\n",
    "                    sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "                    # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "                    features = sub_section.find_element(\n",
    "                        By.CLASS_NAME, 'JxT10f')\n",
    "                    feature_list = (features.text).split('\\n')\n",
    "\n",
    "                    # Getting the table associated with the sub section\n",
    "                    tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "                    feature_dict = {}\n",
    "\n",
    "                    # Adding the date of scrapping\n",
    "                    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                    # Going through every row of every column and creating the list of values associated with each feature\n",
    "                    rows = tables.find_elements(By.TAG_NAME, 'tr')\n",
    "                    for i in range(len(feature_list)):\n",
    "                        feature_dict[feature_list[i]] = []\n",
    "                        feature_dict['Date'] = []\n",
    "\n",
    "                        for row in rows:\n",
    "                            value = row.find_element(\n",
    "                                By.XPATH, f'./td[{i+1}]').text\n",
    "                            feature_dict[feature_list[i]].append(value)\n",
    "                            feature_dict['Date'].append(current_date)\n",
    "\n",
    "                    sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "                except Exception as e:\n",
    "                    extra_title = sub_section.find_element(\n",
    "                        By.CLASS_NAME, 'SQoGqa').text + ' '\n",
    "                    del sub_section_dict[title_sub_section]\n",
    "\n",
    "        else:\n",
    "\n",
    "            sub_section.find_element(By.CLASS_NAME, 'JxT10f').click()\n",
    "\n",
    "            # Getting features name as they are different on each urls. Features comes in one long str thet needs to be split\n",
    "            features = sub_section.find_element(By.CLASS_NAME, 'JxT10f')\n",
    "            feature_list = (features.text).split('\\n')\n",
    "\n",
    "            # Getting the table associated with the sub section\n",
    "            tables = sub_section.find_element(By.CLASS_NAME, 'cjpIYY')\n",
    "            feature_dict = {}\n",
    "\n",
    "            # Adding the date of scrapping\n",
    "            current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Going through every row of every column and creating the list of values associated with each feature\n",
    "            rows = tables.find_elements(By.TAG_NAME, 'tr')\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dict[feature_list[i]] = []\n",
    "                feature_dict['Date'] = []\n",
    "\n",
    "                for row in rows:\n",
    "                    value = row.find_element(By.XPATH, f'./td[{i+1}]').text\n",
    "                    feature_dict[feature_list[i]].append(value)\n",
    "                    feature_dict['Date'].append(current_date)\n",
    "\n",
    "            sub_section_dict[title_sub_section] = feature_dict\n",
    "\n",
    "    page_dict = {}\n",
    "    page_dict[title_page] = section_dict\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return page_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_scrapped(urls):\n",
    "    data = []\n",
    "    # urls_done has been created if the script come to a stop for whatever reason. We can look at the last link added here and restart from here.\n",
    "    urls_done = []\n",
    "    for url in urls:\n",
    "        print(url)  # Help to follow the script live\n",
    "        try:\n",
    "            scraping = url_scraper(url)\n",
    "            data.append(scraping)\n",
    "            urls_done.append(url)\n",
    "        except:  # If there is an issue on a url we can just skip it and move on\n",
    "            pass\n",
    "\n",
    "    return data, urls_done[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining urls we want to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls_extractor/urls_extractor/spiders/urls.json', 'r') as file:\n",
    "    datas = json.load(file)\n",
    "\n",
    "urls_to_scrap = []\n",
    "\n",
    "for data in datas:\n",
    "    for key, value in data.items():\n",
    "        urls_to_scrap.append(value)\n",
    "\n",
    "u = urls_to_scrap[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = urls_scrapped(u)\n",
    "scraping = results[0]\n",
    "urls_processed = results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json__saving(page_dict):\n",
    "    json_string = json.dumps(page_dict)\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    with open(f'scrapings/{current_date}_mon_fichier.json', 'w') as datas:\n",
    "        datas.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json__saving(scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we should save scraping to json and save this json on the data base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
